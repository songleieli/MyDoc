###顺丰：
自我介绍；
项目介绍；

BN思想； 
BN(Batch Normalization):
    BN的主要思想就是对每层输入数据做标准化，使其以较小的方差集中在均值附近。
  随着神经网络的深度增加，会出现梯度扩散和梯度爆炸的问题。改变激活函数，比如说ReLu可以较好地避免梯度的问题，BN也可以解决这个问题。


GBDT算法知道多少？Xgboost知道吗？；

GBDT 梯度提升树

GBDT的思想可以用一个通俗的例子解释，假如有个人30岁，我们首先用20岁去拟合，发现损失有10岁，这时我们用6岁去拟合剩下的损失，发现差距还有4岁，第三轮我们用3岁拟合剩下的差距，差距就只有一岁了。如果我们的迭代轮数还没有完，可以继续迭代下面，每一轮迭代，拟合的岁数误差都会减小。



激活函数有哪些？
为什么要用Leaky relu?

逻辑回归为什么要用似然损失函数？

参数随机初始化方法，为什么？





###火光摇曳（创业公司）
自我介绍；
项目介绍；
GBDT细节；
SVM介绍；
SVM：支持向量机

正则化方法：L1和L2正则化为何能够防止过拟合？最好能给出数学解释

答：在训练数据不够多时，或者overtraining时，常常会导致overfitting（过拟合）
其直观的表现如下图所示，随着训练过程的进行，模型复杂度增加，在training data上的error渐渐减小，但是在验证集上的error却反而渐渐增大——因为训练出来的网络过拟合了训练集，对训练集外的数据却不work。

最简单的解释就是加了先验。在数据少的时候，先验知识可以防止过拟合。

通俗说明
抛硬币，推断正面朝上的概率。如果只能抛5次，很可能5次全正面朝上，这样你就得出错误的结论：正面朝上的概率是1--------过拟合！如果你在模型里加正面朝上概率是0.5的先验，结果就不会那么离谱。这其实就是正则。


手撕代码；

###小米
一面：
自我介绍；
项目介绍；
手撕代码；
二面：
自我介绍；
项目介绍；
看你用了LSTM，讲讲LSTM呗？
LSTM为何能够防止梯度消失？
手撕代码；

###京东
自我介绍；
一开始就手撕代码。。；
数学题四道：梯度，朴素贝叶斯计算，CNN卷积结果计算，还有个线性代数的题忘记了。。
Linux命令：删除文件夹命令。。

深度学习基础知识：样本不平衡处理方法，过拟合解决方法，CNN是什么？答四大特性的一个层次堆叠模型，各特性（局部连接，卷积操作，池化操作，层次堆叠）的作用有什么？

样本不平衡往往会导致模型对样本数较多的分类造成过拟合。
针对样本的不平衡问题，有以下几种常见的解决思路：
1.搜集更多的数据
2.改变评判指标
3.对数据进行采样
4.合成样本
5.改变样本权重

CNN，结构：输入层，最大池化层，卷几层（ReLU），最大池化层，卷几层（ReLU）+.....池化层后面是全连接层（Fully Connected Layer, 简称FC-输出层。输出层使用了Softmax激活函数来做图像识别的分类。

项目介绍：
关于翻墙的探讨。。瞎聊
二面：
自我介绍；
项目介绍；

为什么不用带动量的SGD?我用了Adam。
由于下降方向的不同，可能导致不同算法到达完全不同的局部最优点。Adam用的是自适应学习率。
主流的观点认为：Adam等自适应学习率算法对于稀疏数据具有优势，且收敛速度很快；但精调参数的SGD（+Momentum）往往能够取得更好的最终结果。

SGD方法中的高方差振荡使得网络很难稳定收敛，所以有研究者提出了一种称为动量（Momentum）的技术，通过优化相关方向的训练和弱化无关方向的振荡，来加速SGD训练。

讲一下faster RCNN的RPN网络；
SVM知道多少？实现过吗？
参加过比赛吗？（正在参加jData的比赛）说下思路；
手撕代码；